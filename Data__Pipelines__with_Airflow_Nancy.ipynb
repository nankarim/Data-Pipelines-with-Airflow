{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install apache-airflow"
      ],
      "metadata": {
        "id": "g_yhCf3vEc7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.operators.python_operator import PythonOperator\n",
        "from datetime import datetime, timedelta\n",
        "import pandas as pd\n",
        "from sqlalchemy import create_engine\n",
        "import logging"
      ],
      "metadata": {
        "id": "GVLzRKTzD_B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default_args = {\n",
        "    'owner': 'MTN Rwanda',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2023, 4, 18),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5)\n",
        "}\n",
        "\n",
        "dag = DAG('data_pipeline', default_args=default_args, schedule_interval='@daily')"
      ],
      "metadata": {
        "id": "zru4qTM3E3JW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Data Extraction\n",
        "def extract_data():\n",
        "    customer_data = pd.read_csv('customer_data.csv')\n",
        "    order_data = pd.read_csv('order_data.csv')\n",
        "    payment_data = pd.read_csv('payment_data.csv')\n",
        "    \n",
        "    # load the CSV data into Pandas dataframes\n",
        "    \n",
        "    return customer_data, order_data, payment_data"
      ],
      "metadata": {
        "id": "ZkuTN0ohFQW3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # Data Transformation \n",
        "def transform_data(customer_data, order_data, payment_data):\n",
        "        \n",
        "    # convert date fields to the correct format using pd.to_datetime\n",
        "    customer_data['date_of_birth'] = pd.to_datetime(customer_data['date_of_birth'])\n",
        "    order_data['order_date'] = pd.to_datetime(order_data['order_date'])\n",
        "    payment_data['payment_date'] = pd.to_datetime(payment_data['payment_date'])\n",
        "    \n",
        "    # merging the data\n",
        "    customer_order_data = pd.merge(customer_data, order_data, on='customer_id')\n",
        "    \n",
        "    # merge payment dataframe with the merged dataframe on the order_id and customer_id columns\n",
        "    payment_customer_order_data = pd.merge(payment_data, customer_order_data, on=['order_id', 'customer_id'])\n",
        "    \n",
        "    # drop unnecessary columns like customer_id and order_id.axis \n",
        "    payment_customer_order_data.drop(['customer_id', 'order_id'], axis=1, inplace=True)\n",
        "    \n",
        "    # group the data by customer and aggregate the amount paid using sum\n",
        "    customer_lifetime_value = payment_customer_order_data.groupby(['email', 'country', 'gender', 'date_of_birth']).agg(total_amount_paid=('amount', 'sum'), number_of_orders=('product', 'count')).reset_index()\n",
        "    \n",
        "    # create a new column to calculate the total value of orders made by each customer\n",
        "    customer_lifetime_value['total_order_value'] = customer_lifetime_value['total_amount_paid'] / customer_lifetime_value['number_of_orders']\n",
        "    \n",
        "    # calculate the customer lifetime value using the formula CLV = (average order value) x (number of orders made per year) x (average customer lifespan)\n",
        "    customer_lifetime_value['lifespan'] = (pd.Timestamp.today() - customer_lifetime_value['date_of_birth']).dt.days / 365.25\n",
        "    customer_lifetime_value['average_order_value'] = customer_lifetime_value['total_order_value']\n",
        "    customer_lifetime_value['clv'] = customer_lifetime_value['average_order_value'] * customer_lifetime_value['number_of_orders'] * customer_lifetime_value['lifespan']\n",
        "    \n",
        "    return customer_lifetime_value"
      ],
      "metadata": {
        "id": "_uDUZul2FVb2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Loading \n",
        "def load_data(transformed_data):\n",
        "    try:\n",
        "        \n",
        "       # load the transformed data into Postgres database\n",
        "        engine = create_engine('postgresql://username:password@localhost:5432/dbname')\n",
        "        connection = engine.connect()\n",
        "        table_name = 'mtnrwanda_data'\n",
        "\n",
        "        # create table if it doesn't exist\n",
        "        if not engine.has_table(table_name):\n",
        "            transformed_data.iloc[0:0].to_sql(table_name, engine, if_exists='replace', index=False)\n",
        "\n",
        "        # insert data into table\n",
        "        transformed_data.to_sql(table_name, connection, if_exists='append', index=False)\n",
        "\n",
        "        connection.close()\n",
        "        \n",
        "        logging.info('Data loaded into Postgres database')\n",
        "        \n",
        "    except Exception as e:\n",
        "        logging.error(f'Error loading data into Postgres database: {e}')\n",
        "        raise\n",
        "\n",
        "\n",
        "with dag:\n",
        "    \n",
        "    extract_data_task = PythonOperator(\n",
        "        task_id='extract_data',\n",
        "        python_callable=extract_data\n",
        "    )\n",
        "\n",
        "    transform_data_task = PythonOperator(\n",
        "        task_id='transform_data',\n",
        "        python_callable=transform_data\n",
        "    )\n",
        "\n",
        "    load_data_task = PythonOperator(\n",
        "        task_id='load_data',\n",
        "        python_callable=load_data\n",
        "    )\n",
        "\n",
        "    # define dependencies\n",
        "    extract_data_task >> transform_data_task >> load_data_task"
      ],
      "metadata": {
        "id": "VnZoe21oLvb2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pipeline Documentation**\n",
        "\n",
        "The data pipeline consists of three tasks:\n",
        "\n",
        "**extract_data_task**: reads in CSV files containing customer, order, and payment data and returns them as Pandas dataframes. \n",
        "**transform_data_task**: takes the three dataframes as inputs and performs data cleaning and transformation operations, such as merging, grouping, and aggregating, to compute the customer lifetime value for each customer. **load_data_task**: takes the transformed data as input and loads it into a Postgres database. The data is inserted into a table named \"mtnrwanda_data\". The three tasks are defined as PythonOperator instances in an Airflow DAG named \"data_pipeline\". The DAG is scheduled to run daily.\n",
        "\n",
        "**Best practices**:\n",
        "\n",
        "**Modularization**: The pipeline is split into three tasks, each responsible for a specific part of the data processing. This approach makes the code easier to understand, test, and maintain.\n",
        "\n",
        "**Error handling**: The load_data_task is wrapped in a try/except block, which catches any exceptions that may occur during the loading process and logs an error message. This practice makes the pipeline more resilient to errors and helps to prevent data loss.\n",
        "\n",
        "**Logging**: The load_data_task logs its progress and any errors that occur during the data loading process. This practice makes it easier to debug the pipeline and monitor its performance.\n",
        "\n",
        "**Recommendations for deployment and running the pipeline in a cloud-based provider: **\n",
        "\n",
        "**Use a managed service**: Consider using a managed service like Amazon Redshift or Google BigQuery for your data storage and processing needs. These services are highly scalable, fault-tolerant, and can handle large volumes of data. They also have built-in security and data governance features.\n",
        "\n",
        "**Containerize your application**: Use containers to package your application and its dependencies so that it can be easily deployed on any cloud-based provider. This will allow you to move your application between different environments without worrying about compatibility issues.\n",
        "\n",
        "**Use a managed workflow service**: Use a managed workflow service like AWS Step Functions or Google Cloud Composer to manage the execution of your data pipeline. These services provide a visual interface for building, scheduling, and monitoring workflows, and can automatically retry failed tasks.\n",
        "\n"
      ],
      "metadata": {
        "id": "5HbhP0b2HXhC"
      }
    }
  ]
}